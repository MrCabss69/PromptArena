# Prompt Arena ü§∫ 

Un sistema interactivo para comparar la efectividad de diferentes prompts de ingenier√≠a aplicados a la resoluci√≥n de problemas de c√≥digo utilizando modelos de lenguaje grandes (LLMs). Permite enfrentar dos prompts, generar c√≥digo para un problema espec√≠fico usando OpenRouter o Ollama, realizar una evaluaci√≥n est√°tica del c√≥digo y actualizar un ranking relativo basado en el sistema Elo.

## Caracter√≠sticas Principales

*   **Comparaci√≥n de Prompts:** Introduce dos prompts diferentes para resolver el mismo problema.
*   **Selecci√≥n de Problema:** Elige entre una lista de problemas de programaci√≥n cargados desde una estructura de directorios.
*   **Selecci√≥n de LLM:**
    *   Utiliza modelos populares a trav√©s de la API de **OpenRouter**.
    *   Conecta con un servidor **Ollama** local para usar modelos como Llama 3, Mistral, etc.
*   **Generaci√≥n de C√≥digo:** El LLM seleccionado genera c√≥digo Python basado en cada prompt y la descripci√≥n del problema.
*   **Evaluaci√≥n Est√°tica:** El c√≥digo generado se valida usando `ast.parse` (sintaxis) y `pyflakes` (errores comunes/linting).
*   **Sistema de Ranking Elo:** Los prompts (identificados por su hash) compiten en duelos. Sus ratings Elo se actualizan seg√∫n el resultado de la evaluaci√≥n est√°tica.
*   **Interfaz Web:** Interfaz de usuario sencilla e interactiva construida con **Gradio**.
*   **Extensible:** F√°cil a√±adir nuevos problemas creando una nueva carpeta en `app/core/data/problems/`.

## üõ†Ô∏è Tech Stack

*   **Lenguaje:** Python 3.9+
*   **UI:** Gradio
*   **Modelos de Datos:** Pydantic
*   **Cliente HTTP:** httpx
*   **Evaluaci√≥n de C√≥digo:** ast, pyflakes
*   **LLM Backends:** OpenRouter API, Ollama API

## ‚öôÔ∏è Configuraci√≥n e Instalaci√≥n

1.  **Clonar el Repositorio:**
    ```bash
    git clone <URL_DEL_REPOSITORIO>
    cd VCODEBENCH
    ```

2.  **Crear Entorno Virtual:** (Recomendado)
    ```bash
    python -m venv venv
    source venv/bin/activate  # Linux/macOS
    # venv\Scripts\activate  # Windows
    ```

3.  **Instalar Dependencias:**
    *   Primero, aseg√∫rate de tener un archivo `requirements.txt`. Si no existe, cr√©alo desde tu entorno virtual activo donde ya instalaste las librer√≠as:
        ```bash
        pip freeze > requirements.txt
        ```
    *   Luego, instala las dependencias:
        ```bash
        pip install -r requirements.txt
        ```

4.  **Configurar Variables de Entorno:** Necesitas configurar claves de API y opcionalmente URLs. La forma recomendada es crear un archivo `.env` en la ra√≠z (`VCODEBENCH/`) y a√±adirlo a tu `.gitignore`.

    *   **Crea un archivo `.env`** con el siguiente contenido:
        ```dotenv
        # Obligatorio para usar OpenRouter
        OPENROUTER_API_KEY="tu_clave_api_de_openrouter"

        # Opcional: Si tu servidor Ollama no est√° en http://localhost:11434
        # OLLAMA_BASE_URL="http://tu_ip_o_host:11434"

        # Opcional: Para identificaci√≥n en OpenRouter (puedes dejar los defaults)
        # PROMPT_ARENA_REFERER="https://tu_identificador_web.com"
        # PROMPT_ARENA_TITLE="MiPromptArenaCustom"
        ```
    *   **IMPORTANTE:** Aseg√∫rate de que el archivo `.env` est√© listado en tu `.gitignore` para no subir tus claves secretas a Git.
    *   **Alternativa:** Puedes exportar las variables directamente en tu terminal antes de ejecutar la aplicaci√≥n:
        ```bash
        export OPENROUTER_API_KEY="tu_clave_api_de_openrouter"
        # export OLLAMA_BASE_URL="..." # Si es necesario
        ```

5.  **(Opcional) Configurar Ollama:** Si planeas usar Ollama:
    *   Aseg√∫rate de tener [Ollama](https://ollama.com/) instalado y corriendo.
    *   Descarga los modelos que quieras usar, por ejemplo: `ollama pull llama3`. La interfaz intentar√° listar los modelos disponibles en tu servidor Ollama local.

## ‚ñ∂Ô∏è Ejecuci√≥n

1.  Aseg√∫rate de que tu entorno virtual est√© activado.
2.  Aseg√∫rate de que las variables de entorno est√©n configuradas (ya sea exportadas o mediante un archivo `.env` si usas una librer√≠a como `python-dotenv` - nota: el c√≥digo actual no carga `.env` autom√°ticamente, tendr√≠as que a√±adir `load_dotenv()` al inicio de `app.py`).
3.  Navega al directorio ra√≠z del proyecto (`VCODEBENCH/`).
4.  Ejecuta la aplicaci√≥n Gradio:
    ```bash
    python app/app.py
    ```
5.  Abre tu navegador y ve a la URL local que Gradio indique (normalmente `http://127.0.0.1:7860`).

## üìÅ Estructura del Proyecto

```
VCODEBENCH/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ app.py            # L√≥gica de la interfaz Gradio y flujo principal UI
‚îÇ   ‚îî‚îÄ‚îÄ core/
‚îÇ       ‚îú‚îÄ‚îÄ data/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ problems/ # Directorio base para los problemas
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ <problem_slug>/ # Carpeta para cada problema
‚îÇ       ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ metadata.json     # T√≠tulo, slug
‚îÇ       ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ description.md    # Enunciado del problema
‚îÇ       ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ tests.json        # Casos de prueba
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ       ‚îú‚îÄ‚îÄ elo.py            # L√≥gica de c√°lculo de rating Elo
‚îÇ       ‚îú‚îÄ‚îÄ evaluator.py      # Validaci√≥n est√°tica de c√≥digo (AST, Pyflakes)
‚îÇ       ‚îú‚îÄ‚îÄ llm_client.py     # Cliente para interactuar con OpenRouter y Ollama
‚îÇ       ‚îú‚îÄ‚îÄ models.py         # Modelos de datos Pydantic
‚îÇ       ‚îú‚îÄ‚îÄ problem_bank.py   # Carga y gesti√≥n de problemas desde archivos
‚îÇ       ‚îî‚îÄ‚îÄ runner.py         # Orquesta la comparaci√≥n (LLM -> Eval -> Elo)
‚îú‚îÄ‚îÄ .env.example          # (Recomendado) Ejemplo de variables de entorno
‚îú‚îÄ‚îÄ .gitignore            # Archivos a ignorar por Git
‚îú‚îÄ‚îÄ README.md             # Este archivo
‚îî‚îÄ‚îÄ requirements.txt      # Dependencias de Python
```

## üìù A√±adir Nuevos Problemas

1.  Crea una nueva carpeta dentro de `app/core/data/problems/`. El nombre de la carpeta ser√° el `slug` del problema (ej. `reverse-string`).
2.  Dentro de la nueva carpeta, crea los siguientes archivos:
    *   **`metadata.json`**:
        ```json
        {
          "slug": "reverse-string",
          "title": "Invertir Cadena"
        }
        ```
    *   **`description.md`**: Escribe el enunciado del problema usando Markdown.
    *   **`tests.json`**: Una lista de casos de prueba en formato JSON, donde cada caso es un objeto con `"input"` (una lista de argumentos) y `"expected_output"`:
        ```json
        [
          {"input": ["hello"], "expected_output": "olleh"},
          {"input": [""], "expected_output": ""}
        ]
        ```
3.  La pr√≥xima vez que inicies la aplicaci√≥n, el nuevo problema deber√≠a aparecer en el desplegable.

## üöÄ Futuras Mejoras (Ideas)

*   **Evaluaci√≥n Funcional:** Ejecutar el c√≥digo generado contra los `tests.json` en un entorno seguro (sandbox/Docker).
*   **Persistencia de Ratings:** Guardar los ratings Elo en una base de datos (SQLite, JSON persistente) en lugar de solo en memoria.
*   **Leaderboard de Prompts:** Mostrar un ranking de los prompts con mejor Elo.
*   **Historial de Duelos:** Guardar y mostrar los resultados de comparaciones anteriores.
*   **M√°s M√©tricas de Evaluaci√≥n:** A√±adir complejidad ciclom√°tica, longitud del c√≥digo, etc.
*   **Interfaz Mejorada:** M√°s opciones de filtrado, visualizaciones.# PromptArena
# PromptArena
